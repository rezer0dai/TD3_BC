{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV4vn0TtxwuD"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rezer0dai/TD3_BC/blob/her/td3_bc_her.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBfEjUk1AZf2",
    "outputId": "3cb9b87f-de9f-43bb-fa34-c42a00eeeda1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'TD3_BC' already exists and is not an empty directory.\n",
      "fatal: destination path 'TD3' already exists and is not an empty directory.\n",
      "fatal: destination path 'panda-gym' already exists and is not an empty directory.\n",
      "Obtaining file:///content/panda-gym\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from panda-gym==1.1.0) (0.17.3)\n",
      "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (from panda-gym==1.1.0) (3.1.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from panda-gym==1.1.0) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->panda-gym==1.1.0) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->panda-gym==1.1.0) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->panda-gym==1.1.0) (1.5.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->panda-gym==1.1.0) (0.16.0)\n",
      "Installing collected packages: panda-gym\n",
      "  Attempting uninstall: panda-gym\n",
      "    Found existing installation: panda-gym 1.1.0\n",
      "    Can't uninstall 'panda-gym'. No files were found to uninstall.\n",
      "  Running setup.py develop for panda-gym\n",
      "Successfully installed panda-gym-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rezer0dai/TD3_BC -b her\n",
    "!git clone https://github.com/sfujim/TD3\n",
    "    \n",
    "!git clone https://github.com/qgallouedec/panda-gym\n",
    "\n",
    "!pip install -e panda-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZYrxcvVpAfbB"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "libs = [\"TD3_BC\", \"TD3\", \"panda-gym\"]\n",
    "for lib in libs:\n",
    "    sys.path.append(lib)\n",
    "    sys.path.append(\"/content/\"+lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "guhqvqHMCziy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import utils\n",
    "import TD3_BC\n",
    "import TD3\n",
    "import OurDDPG\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ehdeUdOPyQTs"
   },
   "outputs": [],
   "source": [
    "def eval_policy(policy, eval_env, seed, normalize_state, seed_offset=100, eval_episodes=10):\n",
    "    load_state = lambda obs: obs[\"observation\"].reshape(1,-1)\n",
    "\n",
    "    eval_env.seed(seed + seed_offset)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for z in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            state = load_state(state)\n",
    "            action = policy.select_action(normalize_state(state), train_mode=z < 3)\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Rb9rEEFCyTkQ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "def her(replay_buffer, achieved_goals, her_ratio, n_future, fail_filter, dummy_filter):\n",
    "    if not len(achieved_goals):\n",
    "        return False\n",
    "    leftover = 1 * (replay_buffer.size != len(achieved_goals))\n",
    "    replay_buffer.ptr = replay_buffer.ptr - len(achieved_goals) - leftover\n",
    "    replay_buffer.size = replay_buffer.size - len(achieved_goals) - leftover\n",
    "\n",
    "    if all(np.linalg.norm(achieved_goals[0]- g) < .05 for g in achieved_goals):\n",
    "        return False\n",
    "\n",
    "    norm_ind = replay_buffer.ptr\n",
    "\n",
    "    ep = deepcopy([ (\n",
    "            replay_buffer.state[replay_buffer.ptr + i + leftover],\n",
    "            replay_buffer.action[replay_buffer.ptr + i + leftover],\n",
    "            replay_buffer.next_state[replay_buffer.ptr + i + leftover],\n",
    "            replay_buffer.reward[replay_buffer.ptr + i + leftover],\n",
    "            False\n",
    "            ) for i in range(len(achieved_goals)) ])\n",
    "\n",
    "    for _ in range(config.HER_PER_EP):\n",
    "        ep_ = []\n",
    "        for j, e in enumerate(ep):\n",
    "            s, a, n, r, d = deepcopy(e)\n",
    "            goal = random.choice(achieved_goals[j:][:n_future])\n",
    "            if random.random() < (dummy_filter if (np.linalg.norm(achieved_goals[0] - goal) < .05) else 0.):\n",
    "                continue\n",
    "            \n",
    "            #assert all(achieved_goals[j] == n[:config.GOAL_SIZE]), \"A)failed with {} + {} [{}][{}] <{}>\".format(\n",
    "            #    j, len(ep), achieved_goals[j], n[:config.GOAL_SIZE], np.linalg.norm(achieved_goals[j] - n[:config.GOAL_SIZE])\n",
    "            #    )\n",
    "            \n",
    "            s[-config.GOAL_SIZE:] = deepcopy(goal)\n",
    "            n[-config.GOAL_SIZE:] = deepcopy(goal)\n",
    "            \n",
    "            r = -1. * (np.linalg.norm(achieved_goals[j] - goal) > .05)\n",
    "            if -1 == r and random.random() < fail_filter:\n",
    "                continue\n",
    "            replay_buffer.add(s, a, n, r, d)\n",
    "\n",
    "        if random.random() < her_ratio:\n",
    "            continue\n",
    "\n",
    "        for e in ep:\n",
    "            replay_buffer.add(*e)\n",
    "\n",
    "    #assert all(replay_buffer.not_done[:replay_buffer.size])\n",
    "    #print(\"\\n diff\", norm_ind, replay_buffer.ptr, replay_buffer.ptr-norm_ind, sum(0 == replay_buffer.reward[norm_ind:replay_buffer.ptr]))\n",
    "\n",
    "    #assert all(replay_buffer.not_done[:replay_buffer.size])\n",
    "    replay_buffer.add(s, a, n, r, True)\n",
    "    if len(replay_buffer.state[norm_ind:replay_buffer.ptr]) > 1:# edge of buffer\n",
    "        replay_buffer.normalize_state(replay_buffer.state[norm_ind:replay_buffer.ptr], update=True)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoMV_BJRyZUF",
    "outputId": "2ba3c62f-ca01-4949-8577-4c6219aa3617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Policy: , Env: panda-pusher, Seed: 0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Policy TD3+BC+HER: , Env: panda-pusher, Seed: 0, Observation shape: 30\n",
      "---------------------------------------\n",
      "Epochs : 1.0 [#timestamps : 20532]\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -49.000\n",
      "---------------------------------------\n",
      "Epochs : 2.0 [#timestamps : 40183]\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -49.000\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from open_gym import make_env\n",
    "\n",
    "if True:#__name__ == \"__main__\":\n",
    "    file_name = f\"{config.ENV}_{config.SEED}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: , Env: {config.ENV}, Seed: {config.SEED}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    env = make_env(config.ENV, render=False, colab=True)\n",
    "    eval_env = make_env(config.ENV, render=True, colab=True)\n",
    "\n",
    "    # Set seeds\n",
    "    env.seed(config.SEED)\n",
    "    env.action_space.seed(config.SEED)\n",
    "    torch.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "    \n",
    "    state_dim = env.state_size()\n",
    "    action_dim = env.action_space.shape[0] \n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = { # let it default for td3 and td3+bc\n",
    "            \"state_dim\": state_dim,\n",
    "            \"action_dim\": action_dim,\n",
    "            \"max_action\": max_action,\n",
    "            \"discount\": config.DISCOUNT,\n",
    "            \"tau\": config.TAU,\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "#    policy = TD3.TD3(**kwargs)\n",
    "    policy = TD3_BC.TD3_BC(**kwargs)\n",
    "#    policy = OurDDPG.DDPG(**kwargs)\n",
    "\n",
    "    replay_buffer_a = utils.ReplayBuffer(state_dim, action_dim)\n",
    "    replay_buffer_c = utils.ReplayBuffer(state_dim, action_dim)\n",
    "    replay_buffer_d = utils.ReplayBuffer(state_dim, action_dim)\n",
    "    replay_buffer_e = utils.ReplayBuffer(state_dim, action_dim)\n",
    "    #replay_buffer.ptr = replay_buffer.size = replay_buffer.max_size - 10\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy TD3+BC+HER: , Env: {config.ENV}, Seed: {config.SEED}, Observation shape: {state_dim}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    counter = 0\n",
    "    score = -100\n",
    "\n",
    "    done = True\n",
    "    achieved_goals = []\n",
    "    load_state = lambda obs: obs[\"observation\"].reshape(1,-1)\n",
    "\n",
    "    t = 0\n",
    "    add_prev_exp = 0\n",
    "    total_steps = config.STEPS_PER_EPOCH * config.EPOCHS\n",
    "    while t < total_steps:\n",
    "        counter += 1\n",
    "\n",
    "        if done:\n",
    "            a = her(replay_buffer_a, achieved_goals, her_ratio=1., fail_filter=1., n_future=len(achieved_goals), dummy_filter=.97)\n",
    "            _ = her(replay_buffer_e, achieved_goals, her_ratio=1., fail_filter=.5, n_future=3, dummy_filter=.97)\n",
    "            c = her(replay_buffer_c, achieved_goals, her_ratio=.8, fail_filter=.5, n_future=len(achieved_goals), dummy_filter=.97)\n",
    "            assert a == c\n",
    "\n",
    "            if not a:\n",
    "              replay_buffer_d.size = replay_buffer_d.size - len(achieved_goals)\n",
    "              replay_buffer_d.ptr = replay_buffer_d.ptr - len(achieved_goals)\n",
    "\n",
    "            add_prev_exp = a or c\n",
    "            achieved_goals = []\n",
    "            mc_w = random.randint(1, 5) if random.random() > .7 else 0.\n",
    "\n",
    "            state = load_state(env.reset())\n",
    "\n",
    "        t += add_prev_exp\n",
    "\n",
    "        if t < config.START_STEPS:# or (-1. == reward and random.random() < mc_w / 10.):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = policy.select_action(replay_buffer_a.normalize_state(state))\n",
    "\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        next_state = load_state(observation)\n",
    "        achieved_goals.append(observation[\"achieved_goal\"])\n",
    "\n",
    "        replay_buffer_a.add(state, action, next_state, reward, done)\n",
    "        replay_buffer_c.add(state, action, next_state, reward, done)\n",
    "        if not done: replay_buffer_d.add(state, action, next_state, reward, done)\n",
    "        replay_buffer_e.add(state, action, next_state, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if add_prev_exp and t > config.UPDATE_AFTER and 0 == t % config.UPDATE_EVERY:\n",
    "            for j in range(config.UPDATE_COUNT):\n",
    "                policy.train_critic(replay_buffer_c, batch_size=config.BATCH_SIZE)\n",
    "#                policy.train_critic(replay_buffer_c, batch_size=config.BATCH_SIZE)\n",
    "                policy.train_actor(replay_buffer_d, use_bc=False, batch_size=config.BATCH_SIZE)\n",
    "                if score < -30. and 1 == j % 2:\n",
    "                  policy.train_actor(replay_buffer_a, use_bc=True, batch_size=config.BATCH_SIZE)\n",
    "                elif score > -30. or 0 == j % 2:\n",
    "                  policy.train_actor(replay_buffer_e, use_bc=True, batch_size=config.BATCH_SIZE)\n",
    "            policy.polyak()\n",
    "\n",
    "        \n",
    "        # Evaluate episode\n",
    "        if add_prev_exp and (t + 1) % config.EVAL_FREQ == 0:\n",
    "            print(f\"Epochs : {(t + 1) / config.EVAL_FREQ} [#timestamps : {counter}]\")\n",
    "            score = eval_policy(policy, eval_env, config.SEED, replay_buffer_a.normalize_state)\n",
    "        if score > -10.:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf_EaZ6WydjL"
   },
   "outputs": [],
   "source": [
    "[ eval_policy(policy, eval_env, random.randint(0, 100000), replay_buffer.normalize_state) for _ in range(10) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7ACdQ1Wr8ML"
   },
   "outputs": [],
   "source": [
    "double trainig learn one with bc one w/o, l2 norm only for w/o bc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "td3_bc_her.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
