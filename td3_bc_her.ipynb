{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV4vn0TtxwuD"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rezer0dai/TD3_BC/blob/her/td3_bc_her.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBfEjUk1AZf2",
    "outputId": "a386fbb4-246f-46cf-c979-a37a955b8e2b"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/rezer0dai/TD3_BC -b her\n",
    "!git clone https://github.com/sfujim/TD3\n",
    "    \n",
    "!git clone https://github.com/qgallouedec/panda-gym\n",
    "\n",
    "!pip install -e panda-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYrxcvVpAfbB"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "libs = [\"TD3_BC\", \"TD3\", \"panda-gym\"]\n",
    "for lib in libs:\n",
    "    sys.path.append(lib)\n",
    "    sys.path.append(\"/content/\"+lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guhqvqHMCziy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import utils\n",
    "import TD3_BC\n",
    "import TD3\n",
    "import OurDDPG\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehdeUdOPyQTs"
   },
   "outputs": [],
   "source": [
    "def eval_policy(policy, eval_env, seed, normalize_state, seed_offset=100, eval_episodes=10):\n",
    "    load_state = lambda obs: obs[\"observation\"].reshape(1,-1)\n",
    "\n",
    "    eval_env.seed(seed + seed_offset)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            state = load_state(state)\n",
    "            action = policy.select_action(normalize_state(state))\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb9rEEFCyTkQ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "def her(replay_buffer, achieved_goals):\n",
    "    if not len(achieved_goals):\n",
    "        return False\n",
    "    leftover = 1 * (replay_buffer.size != len(achieved_goals))\n",
    "    replay_buffer.ptr = replay_buffer.ptr - len(achieved_goals) - leftover\n",
    "    replay_buffer.size = replay_buffer.size - len(achieved_goals) - leftover\n",
    "\n",
    "    if all(np.linalg.norm(achieved_goals[0]- g) < .05 for g in achieved_goals):\n",
    "        return False\n",
    "\n",
    "    norm_ind = replay_buffer.ptr\n",
    "\n",
    "    ep = deepcopy([ (\n",
    "            replay_buffer.state[replay_buffer.ptr + i + leftover],\n",
    "            replay_buffer.action[replay_buffer.ptr + i + leftover],\n",
    "            replay_buffer.next_state[replay_buffer.ptr + i + leftover],\n",
    "            replay_buffer.reward[replay_buffer.ptr + i + leftover],\n",
    "            False\n",
    "            ) for i in range(len(achieved_goals)) ])\n",
    "\n",
    "    for _ in range(config.HER_PER_EP):\n",
    "        ep_ = []\n",
    "        for j, e in enumerate(ep):\n",
    "            s, a, n, r, d = deepcopy(e)\n",
    "            goal = random.choice(achieved_goals[j:])\n",
    "            if random.random() < (.9 if (np.linalg.norm(achieved_goals[0] - goal) < .05) else 0.):\n",
    "                continue\n",
    "            \n",
    "            #assert all(achieved_goals[j] == n[:config.GOAL_SIZE]), \"A)failed with {} + {} [{}][{}] <{}>\".format(\n",
    "            #    j, len(ep), achieved_goals[j], n[:config.GOAL_SIZE], np.linalg.norm(achieved_goals[j] - n[:config.GOAL_SIZE])\n",
    "            #    )\n",
    "            \n",
    "            s[-config.GOAL_SIZE:] = deepcopy(goal)\n",
    "            n[-config.GOAL_SIZE:] = deepcopy(goal)\n",
    "            \n",
    "            r = -1. * (np.linalg.norm(achieved_goals[j] - goal) > .05)\n",
    "            if -1 == r and random.random() < .8:\n",
    "                continue\n",
    "            replay_buffer.add(s, a, n, r, d)\n",
    "\n",
    "        if random.random() < config.HER_RATIO:\n",
    "            continue\n",
    "\n",
    "        for e in ep:\n",
    "            replay_buffer.add(*e)\n",
    "\n",
    "    #assert all(replay_buffer.not_done[:replay_buffer.size])\n",
    "    #print(\"\\n diff\", norm_ind, replay_buffer.ptr, replay_buffer.ptr-norm_ind, sum(0 == replay_buffer.reward[norm_ind:replay_buffer.ptr]))\n",
    "\n",
    "    #assert all(replay_buffer.not_done[:replay_buffer.size])\n",
    "    replay_buffer.add(s, a, n, r, True)\n",
    "    if len(replay_buffer.state[norm_ind:replay_buffer.ptr]) > 1:# edge of buffer\n",
    "        replay_buffer.normalize_state(replay_buffer.state[norm_ind:replay_buffer.ptr], update=True)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoMV_BJRyZUF",
    "outputId": "0b70ce92-bcbd-4f70-d56b-c58bc83c0b87"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from open_gym import make_env\n",
    "\n",
    "if True:#__name__ == \"__main__\":\n",
    "    file_name = f\"{config.ENV}_{config.SEED}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: , Env: {config.ENV}, Seed: {config.SEED}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    env = make_env(config.ENV, render=False, colab=True)\n",
    "    eval_env = make_env(config.ENV, render=True, colab=True)\n",
    "\n",
    "    # Set seeds\n",
    "    env.seed(config.SEED)\n",
    "    env.action_space.seed(config.SEED)\n",
    "    torch.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "    \n",
    "    state_dim = env.state_size()\n",
    "    action_dim = env.action_space.shape[0] \n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = { # let it default for td3 and td3+bc\n",
    "            \"state_dim\": state_dim,\n",
    "            \"action_dim\": action_dim,\n",
    "            \"max_action\": max_action,\n",
    "            \"discount\": config.DISCOUNT,\n",
    "            \"tau\": config.TAU,\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "#    policy = TD3.TD3(**kwargs)\n",
    "    policy = TD3_BC.TD3_BC(**kwargs)\n",
    "#    policy = OurDDPG.DDPG(**kwargs)\n",
    "\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "    #replay_buffer.ptr = replay_buffer.size = replay_buffer.max_size - 10\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy TD3+BC+HER: , Env: {config.ENV}, Seed: {config.SEED}, Observation shape: {state_dim}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    done = True\n",
    "    achieved_goals = []\n",
    "    load_state = lambda obs: obs[\"observation\"].reshape(1,-1)\n",
    "\n",
    "    t = 0\n",
    "    add_prev_exp = 0\n",
    "    total_steps = config.STEPS_PER_EPOCH * config.EPOCHS\n",
    "    while t < total_steps:\n",
    "\n",
    "        if done:\n",
    "            add_prev_exp = her(replay_buffer, achieved_goals)\n",
    "            achieved_goals = []\n",
    "            mc_w = random.randint(1, 5) if random.random() > .3 else 0.\n",
    "            state = load_state(env.reset())\n",
    "\n",
    "        t += add_prev_exp\n",
    "\n",
    "        if t < config.START_STEPS or (-1. == reward and random.random() < mc_w / 10.):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = policy.select_action(replay_buffer.normalize_state(state))\n",
    "\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        next_state = load_state(observation)\n",
    "        achieved_goals.append(observation[\"achieved_goal\"])\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if t > config.UPDATE_AFTER and 0 == t % config.UPDATE_EVERY:\n",
    "            print(\"LEARN\")\n",
    "            for j in range(config.UPDATE_COUNT):\n",
    "                policy.train(replay_buffer, config.BATCH_SIZE)\n",
    "            policy.polyak()\n",
    "\n",
    "        score = -100\n",
    "        \n",
    "        # Evaluate episode\n",
    "        if (t + 1) % config.EVAL_FREQ == 0:\n",
    "            print(f\"Epochs : {(t + 1) / config.EVAL_FREQ}\")\n",
    "            score = eval_policy(policy, eval_env, config.SEED, replay_buffer.normalize_state)\n",
    "        if score > -10.:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf_EaZ6WydjL"
   },
   "outputs": [],
   "source": [
    "[ eval_policy(policy, eval_env, random.randint(0, 100000), replay_buffer.normalize_state) for _ in range(10) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7ACdQ1Wr8ML"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "td3_bc_her.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
