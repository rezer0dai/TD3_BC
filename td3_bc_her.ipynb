{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV4vn0TtxwuD"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rezer0dai/TD3_BC/blob/her/td3_bc_her.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBfEjUk1AZf2"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/rezer0dai/TD3_BC -b her\n",
    "#!git clone https://github.com/sfujim/TD3\n",
    "    \n",
    "# her test : https://github.com/sumitsk/HER\n",
    "#!git clone https://github.com/qgallouedec/panda-gym\n",
    "\n",
    "#!pip install -e panda-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYrxcvVpAfbB"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "libs = [\"TD3_BC\", \"TD3\", \"panda-gym\"]\n",
    "for lib in libs:\n",
    "    sys.path.append(lib)\n",
    "    sys.path.append(\"/content/\"+lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guhqvqHMCziy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import utils\n",
    "import TD3_BC\n",
    "#import TD3\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehdeUdOPyQTs"
   },
   "outputs": [],
   "source": [
    "def eval_policy(policy, eval_env, seed, normalize_state, seed_offset=100, eval_episodes=10):\n",
    "    load_state = lambda obs: obs[\"observation\"].reshape(1,-1)\n",
    "\n",
    "    eval_env.seed(seed + seed_offset)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            state = load_state(state)\n",
    "            action = policy.select_action(normalize_state(state))\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb9rEEFCyTkQ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def her(replay_buffer):\n",
    "    ep = []\n",
    "    i = replay_buffer.ptr\n",
    "\n",
    "    if not i:\n",
    "        return\n",
    "\n",
    "    while replay_buffer.not_done[i-len(ep)-2]:\n",
    "        ep.append([\n",
    "            replay_buffer.state[i-len(ep)-1].copy(), \n",
    "            replay_buffer.action[i-len(ep)-1].copy(), \n",
    "            replay_buffer.next_state[i-len(ep)+1].copy(), \n",
    "            replay_buffer.reward[i-len(ep)+1], 0 == len(ep)\n",
    "            ])\n",
    "\n",
    "    for _ in range(config.HER_PER_EP):\n",
    "        ep_ = []\n",
    "        for j, e in enumerate(ep[1:]):\n",
    "            s, a, n, r, d = e\n",
    "            s, n = s.copy(), n.copy()\n",
    "            s[-3:] = n[-3:] = random.choice(ep[:j+1])[0][:3].copy()\n",
    "            r = -1. * (np.linalg.norm(n[:3] - s[-3:]) > .05)\n",
    "            ep_.append([s, a, n, r, d])\n",
    "\n",
    "        for e in ep_:\n",
    "            replay_buffer.add(*e)\n",
    "\n",
    "        if random.random() < config.HER_RATIO:\n",
    "            continue\n",
    "        for e in ep_:\n",
    "            replay_buffer.add(*e)\n",
    "\n",
    "    replay_buffer.add(*(ep_[0][:-1]), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoMV_BJRyZUF"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from open_gym import make_env\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_name = f\"{config.ENV}_{config.SEED}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: , Env: {config.ENV}, Seed: {config.SEED}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    env = make_env(config.ENV, render=False, colab=True)\n",
    "    eval_env = make_env(config.ENV, render=True, colab=True)\n",
    "\n",
    "    # Set seeds\n",
    "    env.seed(config.SEED)\n",
    "    env.action_space.seed(config.SEED)\n",
    "    torch.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "    \n",
    "    state_dim = env.state_size()\n",
    "    action_dim = env.action_space.shape[0] \n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = {\n",
    "            \"state_dim\": state_dim,\n",
    "            \"action_dim\": action_dim,\n",
    "            \"max_action\": max_action,\n",
    "            \"discount\": config.DISCOUNT,\n",
    "            \"tau\": config.TAU,\n",
    "            # TD3\n",
    "            \"policy_noise\": config.POLICY_NOISE * max_action,\n",
    "            \"noise_clip\": config.NOISE_CLIP * max_action,\n",
    "            \"policy_freq\": config.POLICY_FREQ,\n",
    "            # TD3 + BC\n",
    "            \"alpha\": config.ALPHA\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "    #policy = TD3.TD3(**kwargs)\n",
    "    policy = TD3_BC.TD3_BC(**kwargs)\n",
    "\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy TD3+BC+HER: , Env: {config.ENV}, Seed: {config.SEED}, Observation shape: {state_dim}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    load_state = lambda obs: obs[\"observation\"].reshape(1,-1)\n",
    "\n",
    "    done = True\n",
    "    reward = -1.\n",
    "    total_steps = config.STEPS_PER_EPOCH * config.EPOCHS\n",
    "    for t in range(total_steps):\n",
    "\n",
    "        if done:\n",
    "            x = random.randint(0, 5)\n",
    "            her(replay_buffer)\n",
    "            state = load_state(env.reset())\n",
    "\n",
    "        if t < config.START_STEPS or (-1 == reward and random.random() < x / 10.):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = policy.select_action(replay_buffer.normalize_state(state))\n",
    "\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        next_state = load_state(observation)\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if t > config.UPDATE_AFTER and 0 == t % config.UPDATE_EVERY:\n",
    "            for j in range(config.UPDATE_EVERY):\n",
    "                policy.train(replay_buffer, config.BATCH_SIZE)\n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % config.EVAL_FREQ == 0:\n",
    "            print(f\"Time steps: {t+1}\")\n",
    "            eval_policy(policy, eval_env, config.SEED, replay_buffer.normalize_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf_EaZ6WydjL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "td3_bc_her.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
