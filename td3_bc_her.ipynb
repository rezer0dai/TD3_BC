{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "td3_bc_her.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV4vn0TtxwuD"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rezer9dai/blob/her/td3_bc_her.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBfEjUk1AZf2"
      },
      "source": [
        "!git clone https://github.com/rezer0dai/TD3_BC -b her\n",
        "\n",
        "!git clone https://github.com/qgallouedec/panda-gym\n",
        "\n",
        "!pip install -e panda-gym\n",
        "!pip install dataclasses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYrxcvVpAfbB"
      },
      "source": [
        "import sys\n",
        "\n",
        "libs = [\"TD3_BC\", \"panda-gym\"]\n",
        "for lib in libs:\n",
        "    sys.path.append(lib)\n",
        "    sys.path.append(\"/content/\"+lib)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guhqvqHMCziy"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import utils\n",
        "import TD3_BC"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehdeUdOPyQTs"
      },
      "source": [
        "def eval_policy(policy, eval_env, seed, mean, std, seed_offset=100, eval_episodes=10):\n",
        "    load_state = lambda obs: (obs[\"observation\"].reshape(1,-1) - mean)/std\n",
        "\n",
        "    eval_env.seed(seed + seed_offset)\n",
        "\n",
        "    avg_reward = 0.\n",
        "    for _ in range(eval_episodes):\n",
        "        state, done = eval_env.reset(), False\n",
        "        while not done:\n",
        "            state = load_state(state)\n",
        "            action = policy.select_action(state)\n",
        "            state, reward, done, _ = eval_env.step(action)\n",
        "            avg_reward += reward\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "\n",
        "    print(\"---------------------------------------\")\n",
        "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "    print(\"---------------------------------------\")\n",
        "    return avg_reward\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb9rEEFCyTkQ"
      },
      "source": [
        "import random\n",
        "def her(replay_buffer):\n",
        "    ep = []\n",
        "    i = replay_buffer.ptr\n",
        "\n",
        "    if not i:\n",
        "        return\n",
        "\n",
        "    while replay_buffer.not_done[i-len(ep)-2]:\n",
        "        ep.append([\n",
        "            replay_buffer.state[i-len(ep)-1], replay_buffer.action[i-len(ep)-1], replay_buffer.next_state[i-len(ep)+1], replay_buffer.reward[i-len(ep)+1], 0 == len(ep)\n",
        "            ])\n",
        "\n",
        "    for _ in range(20):\n",
        "        ep_ = []\n",
        "        for j, e in enumerate(ep[1:]):\n",
        "            s, a, n, r, d = e\n",
        "            s, n = s.copy(), n.copy()\n",
        "            s[-3:] = n[-3:] = random.choice(ep[:j+1])[0][:3]\n",
        "            r = -1. * (np.linalg.norm(n[:3] - s[-3:]) > .05)\n",
        "            ep_.append([s, a, n, r, d])\n",
        "\n",
        "        for e in ep_:\n",
        "            replay_buffer.add(*e)\n",
        "    replay_buffer.add(*ep[0])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoMV_BJRyZUF"
      },
      "source": [
        "from config import Config\n",
        "from open_gym import make_env\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = Config()\n",
        "    \n",
        "    file_name = f\"{cfg.env}_{cfg.seed}\"\n",
        "    print(\"---------------------------------------\")\n",
        "    print(f\"Policy: , Env: {cfg.env}, Seed: {cfg.seed}\")\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "    env = make_env(cfg.env, render=True, colab=True)\n",
        "\n",
        "    # Set seeds\n",
        "    env.seed(cfg.seed)\n",
        "    env.action_space.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    np.random.seed(cfg.seed)\n",
        "    \n",
        "    state_dim = env.state_size()\n",
        "    action_dim = env.action_space.shape[0] \n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    kwargs = {\n",
        "            \"state_dim\": state_dim,\n",
        "            \"action_dim\": action_dim,\n",
        "            \"max_action\": max_action,\n",
        "            \"discount\": cfg.discount,\n",
        "            \"tau\": cfg.tau,\n",
        "            # TD3\n",
        "            \"policy_noise\": cfg.policy_noise * max_action,\n",
        "            \"noise_clip\": cfg.noise_clip * max_action,\n",
        "            \"policy_freq\": cfg.policy_freq,\n",
        "            # TD3 + BC\n",
        "            \"alpha\": cfg.alpha\n",
        "    }\n",
        "\n",
        "    # Initialize policy\n",
        "    policy = TD3_BC.TD3_BC(**kwargs)\n",
        "\n",
        "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
        "    if cfg.normalize:\n",
        "        mean,std = replay_buffer.normalize_states() \n",
        "    else:\n",
        "        mean,std = 0,1\n",
        "\t\n",
        "\n",
        "    print(\"---------------------------------------\")\n",
        "    print(f\"Policy TD+BC+HER: , Env: {cfg.env}, Seed: {cfg.seed}, Observation shape: {state_dim}\")\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "    load_state = lambda obs: (obs[\"observation\"].reshape(1,-1) - mean)/std\n",
        "\n",
        "    done = True\n",
        "    total_steps = cfg.steps_per_epoch * cfg.epochs\n",
        "    for t in range(total_steps):\n",
        "\n",
        "        if done:\n",
        "            her(replay_buffer)\n",
        "            state = load_state(env.reset())\n",
        "\n",
        "        if t > cfg.start_steps:\n",
        "            action = policy.select_action(state)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        next_state = load_state(observation)\n",
        "\n",
        "        replay_buffer.add(state, action, next_state, reward, done)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if t > cfg.update_after and 0 == t % cfg.update_every:\n",
        "            for j in range(cfg.update_every):\n",
        "                policy.train(replay_buffer, cfg.batch_size)\n",
        "\n",
        "        # Evaluate episode\n",
        "        if (t + 1) % cfg.eval_freq == 0:\n",
        "            print(f\"Time steps: {t+1}\")\n",
        "            eval_policy(policy, env, cfg.seed, mean, std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf_EaZ6WydjL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
